\subsection{Sobre la elección de $k$ para kNN}

kNN desde un punto de vista implementativo es simple y redunda un análisis matemático mayor. No obstante la relación que hay entre $k$ y la cantidad de elementos que están representados en el espacio $n$ dimensional es de interés, no solo para ver si se puede obtener una caracterización del mismo si no para entender si puede ser acotado, lo que nos permitiría reducir regiones donde buscarlo.

En nuestro caso en particular la acción de escribir un número tiene pocas ambigüedades: el dígito a escribir suele ser escrito de contadas formas y es muy fácilmente reconocible en contraste por ejemplo al problema de clasificar sentimientos en el cual mediante el análisis de un texto que puede ser vago hay que determinar si tiene una connotación positiva o negativa (por ejemplo).

Derivada de la ambigüedad o falta de ella, la vectorización tiene un rol preponderante a la hora de decidir donde se van a ubicar estos elementos en el espacio: si la vectorización es buena y tiene en cuenta las ambigüedades que pueden existir como en nuestro caso, entonces todos los elementos pertenecientes a una clase estarán más cercanos.

Dos cúmulos de elementos cercanos en nuestro espacio, podrían considerarse parecidos, como pueden ser las asociadas al $1$ y al $7$. Pero al ser nuestra vectorización ``buena'' (porque tiene en cuenta las ambigüedades y el dominio del problema es simple), podría considerarse poco probable que elementos de uno sean contados como elementos de otra clase si el $k$ es pequeño. Esto siempre que la cantidad de elementos de ambas clases sean parecidas.

Esto nos lleva a la cuestión planteada al principio de la sección.

En resumen: si $C$ es el conjunto de clases sobre las que clasificar y $E$ el conjunto de elementos para entrenar, las mismas están balanceadas al entrenar, el dominio de nuestro problema podría considerarse fácil y en consecuencia tener una buena vectorización, entonces podría tomarse $k\leq\frac{\#E}{\#C}$ para encontrar la clase de las instancias a validar. Tomar $k=1$ implicaría asumir que el elemento más cercano a nuestra instancia de validación es de la misma clase y para ello lo razonable sería contar con todo lo mencionado. En contraste $k=\#E$  implicaría devolver siempre la clase con mayor presencia (por lo que el estimador estimaría pobremente).

Este mismo análisis se corrobora posteriormente durante la experimentación.

\subsection{Metodología PCA}

 \begin{enumerate}
     \item Calculo de la Matriz de Covarianza :
     
Definimos $X \in R^{nxm}$ como la matriz compuesta por las imágenes del dataset ($x_i \in R^{m}$) en sus filas promediadas. 
\par

\begin{itemize}
	\item $Fila(X) =  \frac{x_i - \mu}{\sqrt{n-1}}$  con $\mu =\frac{\sum^{n} x_i}{n} $
\end{itemize}

Luego calculamos la Matriz de Covarianza que por su definición es simplemente :
\begin{itemize}
	\item $Cov=  X^{t}X $
\end{itemize}

\item Cálculo de los $\alpha$ primeros autovectores : 

Para calcular estos utilizamos el método de la potencia en conjunto con el método de deflación para salvar el problema de que el primero únicamente calcula el autovector con autovalor máximo en módulo.

\begin{algorithm}[H]
\caption{Método de la potencia($matriz$:$A$)}
\begin{algorithmic}[H]
    \State $z \leftarrow InicializarZ()$
    \State $iteraciones \leftarrow 0$
    \While{ !(Convergio?) $and$ iteraciones $<$ maxIteraciones    }  
        \State $old_z \leftarrow z$
        \State $z \leftarrow \frac{Az}{||z ||_2}$
        \State $Convergio? \leftarrow CriterioDeCorte(old_z,z,eps)$
        \State $iteraciones \leftarrow iteraciones +1$
    \EndWhile  
    \State $\lambda \leftarrow vAv^t/v^tv$
    \State
    \Return  $\lambda,z$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Método de Deflación(matriz:$A$,int : $\alpha$)}
\begin{algorithmic}[H]
    \State $Autovectores \leftarrow InicializarMatriz()$
    \State $i \leftarrow 0$
    \While{ $i$ $<$ $\alpha$    }  
        \State $\lambda,v \leftarrow $Método de la potencia$(matriz:$A$)$
        \State $Autovectores[i] \leftarrow v$
        \State $A \leftarrow A - \lambda vv^t$
        \State $i \leftarrow i +1$
    \EndWhile  
    \State
    \Return  $Autovectores$
\end{algorithmic}
\end{algorithm}

\item Reducción de dimensionalidad :

Por último para reducir las dimensiones de nuestro conjunto de datos multiplicamos la matriz proveniente de llamar al Método de Deflación por la orginal.
\begin{algorithm}[H]
\caption{PCA(matriz:$A$,int : $\alpha$)}
\begin{algorithmic}[H]
    \State $V  \leftarrow Metodo de Deflacion(A,\alpha)$
    \State
    \Return  $A V$
\end{algorithmic}
\end{algorithm}

\end{enumerate}

%\subsection{Factibildad de aplicar deflación}

%Nuestro PCA utiliza el Método de Deflación para conseguir sus $\alpha$ autovectores y autovalores, para que este Método funcione hay algunas precondiciones que deben cumplirse.
%\begin{enumerate}
% \item La matriz tenga base ortonormal de autovectores.
% \item Todos sus autovalores sean distintos.
% \item Elección de $z_0$ en Método de la Potencia.
%\end{enumerate}

%5La primera sabemos que se cumple porque nosotros trabajamos sobre una matriz de Covarianza %que sabemos que es simétrica:

%$Cov^t = Cov   X^tX$
%y por haber visto en la teorica sabemos que existe una base ortonormal de autovectores.

\subsection{Detalles de implementación }

En la sección Experimentación analizamos los métodos sobre mismos set de validación y entrenamiento variando sus parámetros, notamos que se hacía muy costoso entrenar en cada iteración para los distintos $k$ en el caso de kNN,y las distintas parejas de  $\{ k, \alpha\}$ para kNN+PCA. Por esa razón decidimos agregar una funcionalidad para guardar el entrenamiento y poder volverlo a usar con otro predictor.
\par
Con kNN cuando probamos con un rango amplio de ks, realizamos una corrida previa con el $k$ máximo y luego guardamos para cada dato sus $k$ máximo vecinos más cercanos , haciendo esto nos ahorramos en las iteraciones siguientes volver a buscar los $k'$ vecinos, lo único que hacemos es iterar sobre esta nueva matriz.
\par
Para kNN+PCA mantenemos la modalidad previa con respecto a los vecinos , pero además corremos primero con el $\alpha $ máximo y nos guardamos la matriz transformada para luego cuando tengamos que iterar sobre $\alpha s $ menores solo hagamos slicing sobre sus columnas.
\par
Esto funciona porque tomando $\alpha_{max}$  :
\par
$X_{pca} = XV  $ siendo $X_{pca}$ la matriz transformada y V formada por los $\alpha_{max}$ autovectores en sus columnas.
Sabemos que nuestra nueva imagen transformada :
\par 
$(X_{pca})_i = fila(X)_iV =$ 
\begin{pmatrix}
x_1v_1 & x_2v_2 & ..& x_2v_{\alpha_{max}}
\end{pmatrix}
\par
En particular si quisieramos la imagen transformada para un $\alpha'$ :
\par
$(X_{pca})_i[0:\alpha'] = 
\begin{pmatrix}
x_1v_1 & x_2v_2 & ..& x_2v_{\alpha'}
\end{pmatrix}











